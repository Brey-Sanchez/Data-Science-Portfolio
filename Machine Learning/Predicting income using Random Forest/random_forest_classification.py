# -*- coding: utf-8 -*-
"""Random Forest Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cFKHFcR8n_nrvAjM3MmNl_82-r7fOLTS
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/income.csv')

pd.set_option('display.max_columns', None,
              'display.max_rows', None)

df.head()

# Checking data types
df.dtypes

# Checking for the number of nulls in each column
df.isnull().sum()

# Checking for any empty strings in the columns
df.apply(lambda x: x == '').sum()

# Checking the values in the numeric columns
df.describe()

df.info()

# Checking the unique values for each of the object columns to see if we need
# to change any of the values
for col in df.columns:
  if df[col].dtype != 'int64':
    print(col, '\n', df[col].unique())
    print()

# Encoding the target variable
df.loc[df.SalStat == ' greater than 50,000', 'SalStat'] = 1
df.loc[df.SalStat == ' less than or equal to 50,000', 'SalStat'] = 0

# Checking that the encoding has been done correctly
df['SalStat'].unique()

# Enconding the gender variable
df.loc[df.gender == ' Male', 'gender'] = 1
df.loc[df.gender == ' Female', 'gender'] = 0

df.gender.unique()

# Replacing the question marks with real nan values
df.loc[df.JobType == ' ?', 'JobType'] = np.nan
df.loc[df.occupation == ' ?', 'occupation'] = np.nan

df.JobType.unique()

df.occupation.unique()

df.occupation.value_counts()

# Creating the dummy columns for all of the categorical variables
# that have more than 2 possible values
dummy_cols = [c for c in df.columns if df[c].dtype != 'int64' and c not in ['gender', 'SalStat']]

dummy_cols

dummy_df = pd.get_dummies(data=df, columns=dummy_cols)

dummy_df.head()

import matplotlib.pyplot as plt
import seaborn as sns

dummy_df.dtypes

# Gender and SalStat need to be numeric
dummy_df[['gender', 'SalStat']] = dummy_df[['gender', 'SalStat']].apply(pd.to_numeric)

dummy_df.dtypes

# Plotting the correlations between all variables
figure, ax = plt.subplots(figsize=(50, 20), dpi=300)

sns.heatmap(dummy_df.corr())

from sklearn.model_selection import train_test_split

# Dividing the features and target variables
X = dummy_df.drop('SalStat', axis='columns')
y = dummy_df['SalStat']

# Splitting the dataset in training and testing portions
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# We have a very imbalanced dataset
df.SalStat.value_counts(normalize=True).plot(kind='bar')

# Casting the y series to integer so we don't get any errors when running the
# model and predicting
y_train = y_train.astype('int')
y_test = y_test.astype('int')

from sklearn.ensemble import RandomForestClassifier

# Training the Random Forest
model = RandomForestClassifier(n_jobs=-1, max_depth=19, max_features=10)
model.fit(X_train, y_train)

# Making the predictions
predictions = model.predict(X_test)

from sklearn.metrics import accuracy_score

# Getting the accuracy score
acc_score = accuracy_score(y_test, predictions)

acc_score

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, predictions)

ConfusionMatrixDisplay(cm).plot()

TP = sum((predictions + y_test) == 2)
FP = sum([pred == 1 and true == 0 for pred, true in zip(predictions, y_test)])
TN = sum((predictions + y_test) == 0)
FN = sum([pred == 0 and true == 1 for pred, true in zip(predictions, y_test)])

# Proportion of predicted positives that are actually positive
# (Salary > 50k)
precision = TP / (TP + FP)

precision

# Proportion of true positives detected by the model
recall = TP / (TP + FN)

recall

# Proportion of true negatives detected by the model
TN / (TN + FP)

# Calculating f1 score
from sklearn.metrics import f1_score
f1_score_ = f1_score(y_test, predictions)

f1_score_

# Creating a dict that stores the scores of our model
model_scores = {'Accuracy': acc_score,
                'Precision': precision,
                'Recall': recall,
                'F1 score': f1_score_,
                'Model': 'Default model'}

model_scores

# Checking what the most importan features are
pd.Series({c: imp for c, imp in zip(X.columns, model.feature_importances_)}).\
sort_values(ascending=False).head()

from imblearn.over_sampling import RandomOverSampler

# Oversampling the minority class to balance the training data
ros = RandomOverSampler(random_state=42)

X_ros, y_ros = ros.fit_resample(X, y)

# Checking the distribution of the target column for the new dataset
y_ros.value_counts(normalize=True)

# Number of duplicate rows in the feature dataset before oversampling
X.duplicated().sum()

# A lot of duplicates introduced by over sampling the minority class
# Most of the new duplicates should be for the minority class (income > 50k)
X_ros.duplicated().sum()

# Splitting the dataset in training and testing portions
X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.3, random_state=42)

y_train = y_train.astype('int')
y_test = y_test.astype('int')

# Initializing and fitting the model
model = RandomForestClassifier(n_jobs=-1,)
model.fit(X_train, y_train)

# Making the predictions
predictions = model.predict(X_test)

# Dict to store the new model's scores
ros_model = {}

# New confusion matrix for the oversampled model
cm = confusion_matrix(y_test, predictions)

ConfusionMatrixDisplay(cm).plot()

# Getting the accuracy score
ros_model['Accuracy'] = accuracy_score(y_test, predictions)

TP = sum((predictions + y_test) == 2)
FP = sum([pred == 1 and true == 0 for pred, true in zip(predictions, y_test)])
TN = sum((predictions + y_test) == 0)
FN = sum([pred == 0 and true == 1 for pred, true in zip(predictions, y_test)])

"""Compare these scores with the previous ones:
- We predict the majority class a bit worse
- We predict the minority class significantly better
"""

# Proportion of predicted positives that are actually positive
# (Salary > 50k)
precision = TP / (TP + FP)

ros_model['Precision'] = precision

# Proportion of true positives detected by the model
recall = TP / (TP + FN)

ros_model['Recall'] = recall

# Proportion of true negatives detected by the model
TN / (TN + FP)

# Proportion of predicted negatives that are actually negatives
TN / (TN + FN)

ros_model['F1 score'] = f1_score(y_test, predictions)
ros_model['Model'] = 'Oversampled model'

ros_model

model_scores

scores_df = pd.DataFrame([model_scores, ros_model])

melted_df = pd.melt(scores_df, 
                    id_vars='Model', var_name='Score_type',
                    value_vars=scores_df.columns[:-1])

melted_df

# Creating a barplot to compare the two models's scores
figure, ax = plt.subplots(figsize=(8, 8), dpi=80)

sns.barplot(data=melted_df, x='Score_type', y='value', hue='Model')

plt.ylim(0, 1)
plt.legend(loc='best')

plt.xlabel('Score type')
plt.ylabel('Value')
plt.title('Comparing the two models')

plt.savefig('Model comparison.png', dpi=300)